{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEMANA 12 - SESIÓN 44 - LAB Aprendizaje Supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice de contenidos\n",
    "1. Antes de empezar\n",
    "\n",
    "2. Reto 1 - Explorar el conjunto de datos\n",
    "\n",
    "    2.0.0.1 Explore los datos a vista de pájaro.\n",
    "    \n",
    "    2.0.0.2 A continuación, evalúe si las columnas de este conjunto de datos están fuertemente correlacionadas.\n",
    "\n",
    "3. Reto 2 - Eliminar la colinealidad de columnas.\n",
    "\n",
    "4. Reto 3 - Manejar los valores perdidos\n",
    "\n",
    "    4.0.0.1 En las celdas siguientes, trate los valores que faltan en el conjunto de datos. Recuerde comentar los fundamentos de sus decisiones.\n",
    "    \n",
    "    4.0.0.2 De nuevo, examine el número de valores que faltan en cada columna.\n",
    "\n",
    "5. Reto 4 - Manejo de datos categóricos WHOIS_*\n",
    "    \n",
    "    5.0.0.1 En las celdas siguientes, fije los valores de los países como se ha indicado anteriormente.\n",
    "    \n",
    "    5.0.0.2 Si un número limitado de valores representa la mayoría de los datos, podemos conservar estos valores principales y volver a etiquetar todos los demás valores poco frecuentes.\n",
    "    \n",
    "    5.0.0.3 Después de comprobarlo, mantengamos los 10 valores principales de la columna y reetiquetemos las demás columnas con OTROS.\n",
    "    \n",
    "    5.0.0.4 En la siguiente celda, elimine ['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE'].\n",
    "\n",
    "6. Reto 5 - Manejar los datos categóricos restantes y convertirlos en ordinales\n",
    "    \n",
    "    6.0.0.1 URL es fácil. Simplemente la eliminaremos porque tiene demasiados valores únicos que no hay forma de consolidar.\n",
    "    \n",
    "    6.0.0.2 Imprima el conteo de valores únicos de CHARSET. Puede ver que sólo hay unos pocos valores únicos. Así que podemos dejarlo como está.\n",
    "    \n",
    "    6.0.0.3 Antes de pensar en su propia solución, no lea las instrucciones que vienen a continuación.\n",
    "\n",
    "7. Desafío 6 - Modelado, predicción y evaluación\n",
    "    \n",
    "    7.0.0.1 En este laboratorio probaremos dos modelos diferentes y compararemos nuestros resultados.\n",
    "    \n",
    "    7.0.0.2 Nuestro segundo algoritmo es DecisionTreeClassifier.\n",
    "    \n",
    "    7.0.0.3 Crearemos otro modelo DecisionTreeClassifier con max_depth=5.\n",
    "\n",
    "8. Bonus Challenge - Escalado de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antes de empezar:\n",
    "- Lee el archivo README.md\n",
    "- Comenta todo lo que puedas y utiliza los recursos del archivo README.md\n",
    "- ¡Feliz aprendizaje!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este laboratorio, exploraremos un conjunto de datos que describe sitios web con diferentes características y los etiqueta como benignos o maliciosos. Utilizaremos algoritmos de aprendizaje supervisado para averiguar qué patrones de características es probable que tengan los sitios web maliciosos y utilizaremos nuestro modelo para predecir sitios web maliciosos.\n",
    "\n",
    "Sus características serán:\n",
    "\n",
    "+ URL: es la identificación anónima de la URL analizada en el estudio\n",
    "+ URL_LENGTH: es el número de caracteres de la URL\n",
    "+ NUMBER_SPECIAL_CHARACTERS: es el número de caracteres especiales identificados en la URL, como, «/», «%», «#», «&», «. “, ”=»\n",
    "+ CHARSET: es un valor categórico y su significado es el estándar de codificación de caracteres (también llamado juego de caracteres).\n",
    "+ SERVER: es un valor categórico y su significado es el sistema operativo del servidor obtenido de la respuesta del paquete.\n",
    "+ CONTENT_LENGTH: representa el tamaño del contenido de la cabecera HTTP.\n",
    "+ WHOIS_COUNTRY: es una variable categórica, sus valores son los países que obtuvimos de la respuesta del servidor (en concreto, nuestro script utilizó la API de Whois).\n",
    "+ WHOIS_STATEPRO: es una variable categórica, sus valores son los estados que obtuvimos de la respuesta del servidor (en concreto, nuestro script utilizó la API de Whois).\n",
    "+ WHOIS_REGDATE: Whois proporciona la fecha de registro del servidor, por tanto, esta variable tiene valores de fecha con formato DD/MM/AAAA HH:MM\n",
    "+ WHOIS_UPDATED_DATE: A través del Whois obtenemos la última fecha de actualización del servidor analizado\n",
    "+ TCP_CONVERSATION_EXCHANGE: Esta variable es el número de paquetes TCP intercambiados entre el servidor y nuestro cliente honeypot\n",
    "+ DIST_REMOTE_TCP_PORT: es el número de puertos detectados y diferentes a TCP\n",
    "+ REMOTE_IPS: esta variable tiene el número total de IPs conectadas al honeypot\n",
    "+ APP_BYTES: es el número de bytes transferidos\n",
    "+ SOURCE_APP_PACKETS: paquetes enviados desde el honeypot al servidor\n",
    "+ REMOTE_APP_PACKETS: paquetes recibidos del servidor\n",
    "+ APP_PACKETS: número total de paquetes IP generados durante la comunicación entre el honeypot y el servidor.\n",
    "+ DNS_QUERY_TIMES: número de paquetes DNS generados durante la comunicación entre el honeypot y el servidor.\n",
    "+ TYPE: es una variable categórica, sus valores representan el tipo de página web analizada, en concreto, 1 es para sitios web maliciosos y 0 para sitios web benignos\n",
    "\n",
    "# Desafío 1 - Explorar el conjunto de datos\n",
    "\n",
    "Empecemos explorando el conjunto de datos. Primero carga el archivo de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv(r\"C:\\Users\\tatan\\Desktop\\repo_course\\Week12_Session44_lab-supervised-learning-es\\website.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore los datos a vista de pájaro.\n",
    "\n",
    "Ahora ya deberías estar muy familiarizado con los procedimientos, así que no te daremos las instrucciones paso a paso. Reflexiona sobre lo que hiciste en los laboratorios anteriores y explora el conjunto de datos.\n",
    "\n",
    "Cosas que buscarás:\n",
    "\n",
    "* ¿Qué aspecto tiene el conjunto de datos?\n",
    "* ¿Cuáles son los tipos de datos?\n",
    "* ¿Qué columnas contienen las características de los sitios web?\n",
    "* ¿Qué columna contiene la característica que vamos a predecir? ¿Cuál es el código de los sitios web benignos frente a los maliciosos?\n",
    "* ¿Necesitamos transformar alguna de las columnas de categórica a ordinal? En caso afirmativo, ¿cuáles son esas columnas?\n",
    "\n",
    "Siéntete libre de añadir celdas adicionales para tus exploraciones. Asegúrate de comentar lo que descubras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the dataset looks like?\n",
    "websites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the data types?\n",
    "websites.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns contain the features of the websites?\n",
    "# Crear la tabla de frecuencias para la columna 'cylinders'\n",
    "server_1 = websites['SERVER'].value_counts()\n",
    "\n",
    "# Mostrar la tabla de frecuencias\n",
    "print(\"Tabla de frecuencias para la columna 'servers':\")\n",
    "print(server_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns contain the features of the websites?\n",
    "# Crear la tabla de frecuencias para la columna 'X'\n",
    "WHOIS_COUNTRY = websites['WHOIS_COUNTRY'].value_counts()\n",
    "\n",
    "# Mostrar la tabla de frecuencias\n",
    "print(\"Tabla de frecuencias para la columna 'WHOIS_COUNTRY':\")\n",
    "print(WHOIS_COUNTRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns contain the features of the websites?\n",
    "# Crear la tabla de frecuencias para la columna 'X'\n",
    "WHOIS_STATEPRO  = websites['WHOIS_STATEPRO'].value_counts()\n",
    "\n",
    "# Mostrar la tabla de frecuencias\n",
    "print(\"Tabla de frecuencias para la columna 'WHOIS_STATEPRO':\")\n",
    "print(WHOIS_STATEPRO )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites[\"WHOIS_COUNTRY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites[\"WHOIS_COUNTRY\"].replace(\"United Kingdom\", \"UK\", inplace=True)\n",
    "websites[\"WHOIS_COUNTRY\"].replace(\"[u'GB'; u'UK']\", \"GB\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites[\"WHOIS_COUNTRY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites[\"WHOIS_COUNTRY\"].replace(\"Cyprus\", \"CY\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "websites[\"WHOIS_COUNTRY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns contain the features of the websites?\n",
    "# Crear la tabla de frecuencias para la columna 'X'\n",
    "APP_PACKETS  = websites['APP_PACKETS'].value_counts()\n",
    "\n",
    "# Mostrar la tabla de frecuencias\n",
    "print(\"Tabla de frecuencias para la columna 'APP PACKETS':\")\n",
    "print(APP_PACKETS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which column contains the feature we will predict? What is the code standing for benign vs malicious websites?\n",
    "type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for bening 1 for maliciuos websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A continuación, evalúe si las columnas de este conjunto de datos están fuertemente correlacionadas.\n",
    "\n",
    "En el laboratorio de aprendizaje supervisado Mushroom que hicimos recientemente, mencionamos que nos preocupa si nuestro conjunto de datos tiene columnas fuertemente correlacionadas porque si es el caso tenemos que elegir ciertos algoritmos de ML en lugar de otros. Ahora tenemos que evaluar esto para nuestro conjunto de datos.\n",
    "\n",
    "Por suerte, la mayoría de las columnas de este conjunto de datos son ordinales, lo que nos facilita mucho las cosas. En las siguientes celdas, evalúe el nivel de colinealidad de los datos.\n",
    "\n",
    "Aquí tienes algunas indicaciones generales que puede consultar para completar este paso:\n",
    "\n",
    "1. Crea una matriz de correlaciones utilizando las columnas numéricas del conjunto de datos.\n",
    "\n",
    "2. Crea un mapa de calor utilizando `seaborn` para visualizar qué columnas tienen una alta colinealidad.\n",
    "\n",
    "3. Comenta qué columnas podría necesitar eliminar debido a la alta colinealidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "cols_num = websites.select_dtypes(include='number').columns\n",
    "cols_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "correlation_matrix = websites[cols_num].corr()\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', square=True, cmap='viridis')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = websites.drop(columns=['TCP_CONVERSATION_EXCHANGE']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num = websites.select_dtypes(include='number').columns\n",
    "cols_num\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "correlation_matrix = websites[cols_num].corr()\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', square=True, cmap='viridis')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este es un ejemplo para conocer la importancia de las características usando un modelo ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "random_state = 42\n",
    "xgb = xgb.XGBClassifier(random_state=random_state)\n",
    "X = websites._get_numeric_data().drop('Type', axis=1)\n",
    "y = websites.Type\n",
    "xgb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = xgb.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.barh(X.columns[sort_idx],xgb.feature_importances_[sort_idx])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    En el gráfico anterior podemos ver las características con menor peso en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío 2 - Eliminar la colinealidad de columnas.\n",
    "\n",
    "En el mapa de calor que ha creado, deberías haber visto al menos 3 columnas que pueden eliminarse debido a la alta colinealidad. Elimina estas columnas del conjunto de datos.\n",
    "\n",
    "Ten en cuenta que debes eliminar el menor número posible de columnas. No tienes que eliminar todas las columnas a la vez. En su lugar, intenta eliminar una columna y, a continuación, vuelve a elaborar el mapa térmico para determinar si deben eliminarse columnas adicionales. Cuando el conjunto de datos ya no contenga columnas correlacionadas en más de un 90%, puedes parar. Además, ten en cuenta que cuando dos columnas tienen una alta colinealidad, sólo necesitas eliminar una de ellas, pero no ambas.\n",
    "\n",
    "En las celdas de abajo, elimina tantas columnas como puedas para eliminar la alta colinealidad en el conjunto de datos. Asegúrate de comentar tu camino para que se pueda conocer tu razonamiento, lo que permitirá dar feedback. Al final, vuelve a imprimir el mapa de calor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites = websites.drop(columns=['APP_PACKETS','REMOTE_APP_BYTES']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE 4 COLUMNS WITH MORE COLLINEARITY\n",
    "cols_num = websites.select_dtypes(include='number').columns\n",
    "cols_num\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "correlation_matrix = websites[cols_num].corr()\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', square=True, cmap='viridis')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 3 - Manejar los valores que faltan\n",
    "\n",
    "El siguiente paso sería manejar los valores faltantes. **Comenzamos examinando el número de valores que faltan en cada columna.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Comprobar si hay valores nulos en el conjunto de datos\n",
    "missing_values = websites.isnull().sum()\n",
    "\n",
    "# Mostrar las columnas con valores nulos\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(missing_values)\n",
    "\n",
    "print('--------------------------------------------')\n",
    "\n",
    "websites.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firts we will drop the columns with more than 50% of missing data\n",
    "websites = websites.drop(columns=['CONTENT_LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will drop the rows with missing data\n",
    "# Eliminar todas las filas con al menos un valor nulo\n",
    "websites = websites.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De nuevo, examina el número de valores que faltan en cada columna. \n",
    "\n",
    "    Si todos están limpios, procede. Si no, vuelve atrás y haz más limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing values in each column\n",
    "# Comprobar si hay valores nulos en el conjunto de datos\n",
    "missing_values = websites.isnull().sum()\n",
    "\n",
    "# Mostrar las columnas con valores nulos\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(missing_values)\n",
    "print('-----------------------------')\n",
    "# Verificar el resultado\n",
    "websites.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 4 - Manejar datos categóricos `WHOIS_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varias columnas categóricas que necesitamos manejar. Estas columnas son:\n",
    "\n",
    "* URL\n",
    "* CHARSET\n",
    "* SERVIDOR\n",
    "* PAÍS\n",
    "* «WHOIS_STATEPRO\n",
    "* WHOIS_REGDATE\n",
    "* WHOIS_UPDATED_DATE\n",
    "\n",
    "La forma de tratar las columnas de cadena es siempre caso por caso. Empecemos trabajando con `WHOIS_COUNTRY`. Tus pasos son:\n",
    "\n",
    "1. Enumera los valores únicos de `WHOIS_COUNTRY`.\n",
    "1. Consolide los valores de país con códigos de país coherentes. Por ejemplo, los siguientes valores se refieren al mismo país y deben utilizar un código de país coherente:\n",
    "    * `CY` y `Cyprus`.\n",
    "    * US y US\n",
    "    * SE y SE\n",
    "    * GB, Reino Unido y GB, Reino Unido.\n",
    "\n",
    "#### En las celdas de abajo, fija los valores de los países como se indica arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "good_country = {'None':'None', \n",
    "                'US':'US', \n",
    "                'SC':'SC', \n",
    "                'GB':'UK', \n",
    "                'UK':'UK', \n",
    "                'RU':'RU', \n",
    "                'AU':'AU', \n",
    "                'CA':'CA',\n",
    "                'PA':'PA',\n",
    "                'se':'SE', \n",
    "                'IN':'IN',\n",
    "                'LU':'LU', \n",
    "                'TH':'TH', \n",
    "                \"[u'GB'; u'UK']\":'UK', \n",
    "                'FR':'FR',\n",
    "                'NL':'NL',\n",
    "                'UG':'UG', \n",
    "                'JP':'JP', \n",
    "                'CN':'CN', \n",
    "                'SE':'SE',\n",
    "                'SI':'SI', \n",
    "                'IL':'IL', \n",
    "                'ru':'RU', \n",
    "                'KY':'KY', \n",
    "                'AT':'AT', \n",
    "                'CZ':'CZ', \n",
    "                'PH':'PH', \n",
    "                'BE':'BE', \n",
    "                'NO':'NO', \n",
    "                'TR':'TR', \n",
    "                'LV':'LV',\n",
    "                'DE':'DE', \n",
    "                'ES':'ES', \n",
    "                'BR':'BR', \n",
    "                'us':'US', \n",
    "                'KR':'KR', \n",
    "                'HK':'HK', \n",
    "                'UA':'UA', \n",
    "                'CH':'CH', \n",
    "                'United Kingdom':'UK',\n",
    "                'BS':'BS', \n",
    "                'PK':'PK', \n",
    "                'IT':'IT', \n",
    "                'Cyprus':'CY', \n",
    "                'BY':'BY', \n",
    "                'AE':'AE', \n",
    "                'IE':'IE', \n",
    "                'UY':'UY', \n",
    "                'KG':'KG'}\n",
    "\n",
    "websites.WHOIS_COUNTRY = websites.WHOIS_COUNTRY.apply(lambda x : good_country[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que hemos fijado los valores de los países, ¿podemos convertir ahora esta columna en ordinal?\n",
    "\n",
    "Todavía no. Si reflexionas, en los laboratorios anteriores sobre cómo manejamos las columnas categóricas, probablemente recuerdes que acabamos eliminando muchas de esas columnas porque hay demasiados valores únicos. Demasiados valores únicos en una columna no es deseable en el aprendizaje automático porque hace que la predicción sea inexacta. Pero hay soluciones bajo ciertas condiciones. Una de las condiciones solucionables es:\n",
    "\n",
    "#### Si un número limitado de valores representa la mayoría de los datos, podemos conservar estos valores principales y volver a etiquetar todos los demás valores poco frecuentes.\n",
    "\n",
    "La columna `WHOIS_COUNTRY` resulta ser este caso. Puedes comprobarlo imprimiendo un gráfico de barras de los `value_counts` en la siguiente celda para verificarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def print_bar_plot(x,y):\n",
    "    plt.bar(x, y)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bar_plot(websites.WHOIS_COUNTRY.unique(),websites.WHOIS_COUNTRY.value_counts());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Después de verificar, ahora vamos a mantener los 10 primeros valores de la columna y volver a etiquetar otras columnas con `OTHER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites[\"WHOIS_COUNTRY\"].replace(websites[\"WHOIS_COUNTRY\"].value_counts().index[10:], \"Other\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bar_plot(websites.WHOIS_COUNTRY.unique(),websites.WHOIS_COUNTRY.value_counts());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que se ha cambiado la etiqueta `WHOIS_COUNTRY`, ya no necesitamos `WHOIS_STATEPRO` porque los valores de los estados o provincias pueden dejar de ser relevantes. Eliminaremos esta columna.\n",
    "\n",
    "Además, también eliminaremos `WHOIS_REGDATE` y `WHOIS_UPDATED_DATE`. Se trata de las fechas de registro y actualización de los dominios del sitio web. No son de nuestra incumbencia.\n",
    "\n",
    "#### En la siguiente celda, elimina `['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites = websites.drop(columns=['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 5 - Manejar los datos categóricos restantes y convertirlos en ordinales\n",
    "\n",
    "Ahora vuelve a imprimir los `dtypes` de los datos. Además de `WHOIS_COUNTRY` que ya hemos arreglado, deberían quedar 3 columnas categóricas: `URL`, `CHARSET`, y `SERVER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `URL` es fácil. Simplemente lo eliminaremos porque tiene demasiados valores únicos que no hay forma de consolidar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites = websites.drop(columns=['URL']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imprime el recuento de valores únicos de `CHARSET`. Usted ve que hay sólo unos pocos valores únicos. Así que podemos dejarlo como está."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Crear la tabla de frecuencias para la columna 'cylinders'\n",
    "charset = websites['CHARSET'].value_counts()\n",
    "\n",
    "# Mostrar la tabla de frecuencias\n",
    "print(\"Tabla de frecuencias para la columna 'charset':\")\n",
    "print(charset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SERVER` es un poco más complicado. Imprime sus valores únicos y piensa cómo puedes consolidar esos valores.\n",
    "\n",
    "#### Antes de pensar en tu propia solución, no leas las instrucciones que vienen a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Crear la tabla de frecuencias para la columna 'cylinders'\n",
    "server_1 = websites['SERVER'].value_counts()\n",
    "\n",
    "# Mostrar la tabla de frecuencias\n",
    "print(\"Tabla de frecuencias para la columna 'servers':\")\n",
    "print(server_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Función para categorizar servidores\n",
    "def categorize_server(server):\n",
    "    server = server.lower()  # Convertimos el texto a minúsculas para que sea insensible a mayúsculas\n",
    "    if 'microsoft' in server:\n",
    "        return 'Microsoft'\n",
    "    elif 'apache' in server:\n",
    "        return 'Apache'\n",
    "    elif 'nginx' in server:\n",
    "        return 'nginx'\n",
    "    else:\n",
    "        return 'Other'\n",
    "# Aplicar la función a la columna SERVER\n",
    "websites['SERVER'] = websites['SERVER'].apply(categorize_server)\n",
    "# Verificar valores únicos después de la transformación\n",
    "print(websites['SERVER'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque hay tantos valores únicos en la columna `SERVER`, en realidad sólo hay 3 tipos principales de servidores: Microsoft, Apache y Nginx. Simplemente comprueba si cada valor de `SERVER` contiene alguno de esos tipos de servidor y vuelve a etiquetarlos. Para los valores `SERVER` que no contengan ninguna de esas subcadenas, etiquétalos con `Other`.\n",
    "\n",
    "Al final, la columna «SERVIDOR» sólo debe contener 4 valores únicos: `Microsoft`, `Apache`, `nginx`, y `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count `SERVER` value counts here\n",
    "server_1 = websites['SERVER'].value_counts()\n",
    "\n",
    "# Mostrar la tabla de frecuencias\n",
    "print(\"Tabla de frecuencias para la columna 'servers':\")\n",
    "print(server_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, todos nuestros datos categóricos están fijados ahora. **Vamos a convertirlos en datos ordinales usando la función `get_dummies` de Pandas ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)). Asegúrate de eliminar las columnas categóricas pasando `drop_first=True` a `get_dummies` ya que no las necesitamos. **Además, asigna los datos con valores ficticios a una nueva variable `website_dummy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites['SERVER'] = pd.Categorical(websites['SERVER']).codes\n",
    "websites['CHARSET'] = pd.Categorical(websites['CHARSET']).codes\n",
    "websites['WHOIS_COUNTRY'] = pd.Categorical(websites['WHOIS_COUNTRY']).codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, inspeccione `website_dummy` para asegurarse de que los datos y tipos son los previstos - no debería haber ninguna columna categórica en este punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "server_1 = websites['SERVER'].value_counts()\n",
    "\n",
    "# Mostrar la tabla de frecuencias\n",
    "print(\"Tabla de frecuencias para la columna 'servers':\")\n",
    "print(server_1)\n",
    "\n",
    "print('----------------------------')\n",
    "server_2 = websites['CHARSET'].value_counts()\n",
    "\n",
    "# Mostrar la tabla de frecuencias\n",
    "print(\"Tabla de frecuencias para la columna 'charset':\")\n",
    "print(server_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío 6 - Modelado, predicción y evaluación\n",
    "\n",
    "Comenzaremos esta sección dividiendo los datos en train y test. **Nombra tus 4 variables `X_entrenamiento`, `X_prueba`, `y_entrenamiento` y `y_prueba`. Selecciona el 80% de los datos para entrenar y el 20% para probar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code here:\n",
    "X = websites.drop(columns=['Type'])  # Variables predictoras\n",
    "y = websites['Type']  # Variable de respuesta (type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=420)\n",
    "# random_state usamos el mismo valor, cada 42 filas lo aparto al train.\n",
    "# Verificar las formas de los conjuntos de datos\n",
    "print(\"Tamaño del conjunto de entrenamiento (X_train, y_train):\", X_train.shape, y_train.shape)\n",
    "print(\"Tamaño del conjunto de prueba (X_test, y_test):\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En este laboratorio, probaremos dos modelos diferentes y compararemos nuestros resultados.\n",
    "\n",
    "El primer modelo que utilizaremos en este laboratorio es la regresión logística. Ya hemos aprendido sobre la regresión logística como algoritmo de clasificación. En la celda de abajo, cargue `LogisticRegression` de scikit-learn e inicialice el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Importar la clase LogisticRegression --> ya lo teníamos de antes! \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Inicializar el modelo de regresión lineal\n",
    "modelo_regresion = LogisticRegression()\n",
    "# Verificar que el modelo ha sido inicializado correctamente\n",
    "print(modelo_regresion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, ajustamos el modelo a nuestros datos de entrenamiento. Ya hemos separado nuestros datos en 4 partes. Utilízalos en tu modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Ajustar el modelo con los datos de entrenamiento\n",
    "modelo_regresion.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "modelo_regresion.fit(X_train, y_train)\n",
    "# Realizar las predicciones sobre el conjunto de prueba\n",
    "y_pred = modelo_regresion.predict(X_test)\n",
    "# Calcular la puntuación R^2\n",
    "r2_score = modelo_regresion.score(X_test, y_test)\n",
    "# Imprimir la puntuación R^2\n",
    "print(f'Puntuación R^2 del modelo: {r2_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, importamos `confusion_matrix` y `accuracy_score` de `sklearn.metrics` y ajustamos nuestros datos de prueba. Asigna los datos ajustados a `y_pred` e imprime la matriz de confusión y la puntuación de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las funciones necesarias\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# Supongamos que ya tienes los datos de prueba y el modelo entrenado\n",
    "# Ejemplo: X_test, y_test son los datos de prueba y model es el modelo entrenado\n",
    "# Realizamos predicciones con el modelo entrenado\n",
    "y_pred = modelo_regresion.predict(X_test)\n",
    "# Calculamos la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# Calculamos la puntuación de precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Imprimimos los resultados\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nPrecisión:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Obtener la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#Crear un mapa de calor para visualizar la matriz de confusión\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "\n",
    "#Agregar etiquetas y títulos\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "#Mostrar la matriz de confusión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos predicciones sobre los datos de prueba\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué opinas del rendimiento del modelo? Escribe tus conclusiones a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuestro segundo algoritmo es DecisionTreeClassifier\n",
    "\n",
    "Aunque no es necesario, vamos a ajustar un modelo utilizando los datos de entrenamiento y luego probar el rendimiento del modelo utilizando los datos de prueba. Empezaremos cargando `DecisionTreeClassifier` de scikit-learn y luego inicializando y ajustando el modelo. Empezaremos con un modelo donde max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred=regressor.predict(X_test)\n",
    "\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "r2=r2_score(y_test,y_pred)\n",
    "\n",
    "print(f\"MSE :{mse}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred=regressor.predict(X_test)\n",
    "\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "r2=r2_score(y_test,y_pred)\n",
    "\n",
    "print(f\"MSE :{mse}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar tu modelo, calcula las probabilidades predichas, decide 0 o 1 utilizando un umbral de 0,5 e imprime la matriz de confusión, así como la puntuación de precisión (en el conjunto de prueba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Importamos las librerías necesarias\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# Supongamos que ya tenemos los datos de entrenamiento y prueba\n",
    "# Ejemplo: X_train, y_train, X_test, y_test\n",
    "# Inicializamos el modelo de árbol de decisión con max_depth=3\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "# Ajustamos el modelo con los datos de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "# Realizamos predicciones con los datos de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculamos la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# Calculamos la puntuación de precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Imprimimos los resultados\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nPrecisión:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizamos la matriz de confusión\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Clase 0', 'Clase 1'], yticklabels=['Clase 0', 'Clase 1'])\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Reales')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a crear otro modelo DecisionTreeClassifier con max_depth=5. \n",
    "Inicia y ajusta el modelo de abajo e imprime la matriz de confusión y la puntuación de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Importamos las librerías necesarias\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# Supongamos que ya tenemos los datos de entrenamiento y prueba\n",
    "# Ejemplo: X_train, y_train, X_test, y_test\n",
    "# Inicializamos el modelo de árbol de decisión con max_depth=3\n",
    "model = DecisionTreeClassifier(max_depth=5)\n",
    "# Ajustamos el modelo con los datos de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "# Realizamos predicciones con los datos de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculamos la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# Calculamos la puntuación de precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Imprimimos los resultados\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nPrecisión:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Has observado una mejora en la matriz de confusión al aumentar max_depth a 5? ¿Has observado una mejora en la puntuación de precisión? Escribe tus conclusiones a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Add your conclusion here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Escalado de características\n",
    "\n",
    "La resolución de problemas en el aprendizaje automático es iterativa. Puede mejorar la predicción de su modelo con diversas técnicas (aunque hay un punto óptimo para el tiempo que invierte y la mejora que obtiene). Ahora sólo has completado una iteración del análisis ML. Hay más iteraciones que puedes realizar para introducir mejoras. Para poder hacerlo, necesitarás conocimientos más profundos en estadística y dominar más técnicas de análisis de datos. En este bootcamp, no tenemos tiempo para alcanzar ese objetivo avanzado. Pero harás esfuerzos constantes después del bootcamp para conseguirlo finalmente.\n",
    "\n",
    "Sin embargo, ahora sí queremos que aprendas una de las técnicas avanzadas que se llama *feature scaling*. La idea del escalado de características es estandarizar/normalizar el rango de variables independientes o características de los datos. Esto puede hacer que los valores atípicos sean más evidentes para que pueda eliminarlos. Este paso debe realizarse durante el Desafío 6 después de dividir los datos de entrenamiento y de prueba, ya que no desea dividir los datos de nuevo, lo que hace imposible comparar los resultados con y sin el escalado de características. Para conceptos generales sobre el escalado de características, haga clic [aquí](https://en.wikipedia.org/wiki/Feature_scaling). Para profundizar, haga clic [aquí](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e).\n",
    "\n",
    "En la siguiente celda, intente mejorar la precisión de predicción de su modelo mediante el escalado de características. Una librería que puedes utilizar es `sklearn.preprocessing.RobustScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)). Utilizarás `RobustScaler` para ajustar y transformar tu `X_train`, y luego transformar `X_test`. Utilizarás la regresión logística para ajustar y predecir tus datos transformados y obtener la puntuación de precisión de la misma manera. Compare la puntuación de precisión con sus datos normalizados con los datos de precisión anteriores. ¿Se ha producido alguna mejora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your comments here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
